{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNqmgSTGGuMC",
        "outputId": "7d20cb03-9315-4aab-c4c3-52158b74e1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ORIGINAL TEXT:\n",
            "================================================================================\n",
            "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence! In this lecture, we\n",
            "explored tokenization – splitting text into words or subwords. We also discussed removing\n",
            "punctuation, converting to lowercase, and eliminating stop words like 'is', 'the', and 'an'. Let's\n",
            "normalize text properly before passing it to a model.\n",
            "\n",
            "Step 1 - Lowercase:\n",
            "natural language processing (nlp) is a fascinating field of artificial intelligence! in this lecture, we\n",
            "explored tokenization – splitting text into words or subwords. we also discussed removing\n",
            "punctuation, converting to lowercase, and eliminating stop words like 'is', 'the', and 'an'. let's\n",
            "normalize text properly before passing it to a model.\n",
            "\n",
            "Step 2 - Digits Removed:\n",
            "natural language processing (nlp) is a fascinating field of artificial intelligence! in this lecture, we\n",
            "explored tokenization – splitting text into words or subwords. we also discussed removing\n",
            "punctuation, converting to lowercase, and eliminating stop words like 'is', 'the', and 'an'. let's\n",
            "normalize text properly before passing it to a model.\n",
            "\n",
            "Step 3 - Punctuation Removed:\n",
            "natural language processing nlp is a fascinating field of artificial intelligence in this lecture we\n",
            "explored tokenization – splitting text into words or subwords we also discussed removing\n",
            "punctuation converting to lowercase and eliminating stop words like is the and an lets\n",
            "normalize text properly before passing it to a model\n",
            "\n",
            "Step 4 - Whitespace Trimmed:\n",
            "natural language processing nlp is a fascinating field of artificial intelligence in this lecture we explored tokenization – splitting text into words or subwords we also discussed removing punctuation converting to lowercase and eliminating stop words like is the and an lets normalize text properly before passing it to a model\n",
            "\n",
            "Step 5 - Tokenized:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', 'in', 'this', 'lecture', 'we', 'explored', 'tokenization', '–', 'splitting', 'text', 'into', 'words', 'or', 'subwords', 'we', 'also', 'discussed', 'removing', 'punctuation', 'converting', 'to', 'lowercase', 'and', 'eliminating', 'stop', 'words', 'like', 'is', 'the', 'and', 'an', 'lets', 'normalize', 'text', 'properly', 'before', 'passing', 'it', 'to', 'a', 'model']\n",
            "\n",
            "Step 6 - Stop Words Removed:\n",
            "['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'artificial', 'intelligence', 'lecture', 'explored', 'tokenization', '–', 'splitting', 'text', 'words', 'subwords', 'also', 'discussed', 'removing', 'punctuation', 'converting', 'lowercase', 'eliminating', 'stop', 'words', 'like', 'lets', 'normalize', 'text', 'properly', 'passing', 'model']\n",
            "\n",
            "Step 7 - Stemmed Tokens:\n",
            "['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'artifici', 'intellig', 'lectur', 'explor', 'token', '–', 'split', 'text', 'word', 'subword', 'also', 'discuss', 'remov', 'punctuat', 'convert', 'lowercas', 'elimin', 'stop', 'word', 'like', 'let', 'normal', 'text', 'properli', 'pass', 'model']\n",
            "\n",
            "Step 8 - Lemmatized Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'artificial', 'intelligence', 'lecture', 'explored', 'tokenization', '–', 'splitting', 'text', 'word', 'subwords', 'also', 'discussed', 'removing', 'punctuation', 'converting', 'lowercase', 'eliminating', 'stop', 'word', 'like', 'let', 'normalize', 'text', 'properly', 'passing', 'model']\n",
            "\n",
            "================================================================================\n",
            "COMPARISON: STEMMING VS LEMMATIZATION\n",
            "================================================================================\n",
            "Original             Stemmed              Lemmatized          \n",
            "------------------------------------------------------------\n",
            "natural              natur                natural              *\n",
            "language             languag              language             *\n",
            "processing           process              processing           *\n",
            "nlp                  nlp                  nlp                 \n",
            "fascinating          fascin               fascinating          *\n",
            "field                field                field               \n",
            "artificial           artifici             artificial           *\n",
            "intelligence         intellig             intelligence         *\n",
            "lecture              lectur               lecture              *\n",
            "explored             explor               explored             *\n",
            "tokenization         token                tokenization         *\n",
            "–                    –                    –                   \n",
            "splitting            split                splitting            *\n",
            "text                 text                 text                \n",
            "words                word                 word                \n",
            "subwords             subword              subwords             *\n",
            "also                 also                 also                \n",
            "discussed            discuss              discussed            *\n",
            "removing             remov                removing             *\n",
            "punctuation          punctuat             punctuation          *\n",
            "converting           convert              converting           *\n",
            "lowercase            lowercas             lowercase            *\n",
            "eliminating          elimin               eliminating          *\n",
            "stop                 stop                 stop                \n",
            "words                word                 word                \n",
            "like                 like                 like                \n",
            "lets                 let                  let                 \n",
            "normalize            normal               normalize            *\n",
            "text                 text                 text                \n",
            "properly             properli             properly             *\n",
            "passing              pass                 passing              *\n",
            "model                model                model               \n",
            "\n",
            "* indicates different results between stemming and lemmatization\n",
            "\n",
            "================================================================================\n",
            "FINAL OUTPUT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Cleaned & Tokenized:\n",
            "['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'artificial', 'intelligence', 'lecture', 'explored', 'tokenization', '–', 'splitting', 'text', 'words', 'subwords', 'also', 'discussed', 'removing', 'punctuation', 'converting', 'lowercase', 'eliminating', 'stop', 'words', 'like', 'lets', 'normalize', 'text', 'properly', 'passing', 'model']\n",
            "\n",
            "Stemmed Tokens:\n",
            "['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'artifici', 'intellig', 'lectur', 'explor', 'token', '–', 'split', 'text', 'word', 'subword', 'also', 'discuss', 'remov', 'punctuat', 'convert', 'lowercas', 'elimin', 'stop', 'word', 'like', 'let', 'normal', 'text', 'properli', 'pass', 'model']\n",
            "\n",
            "Lemmatized Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'artificial', 'intelligence', 'lecture', 'explored', 'tokenization', '–', 'splitting', 'text', 'word', 'subwords', 'also', 'discussed', 'removing', 'punctuation', 'converting', 'lowercase', 'eliminating', 'stop', 'word', 'like', 'let', 'normalize', 'text', 'properly', 'passing', 'model']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#BAI-7B\n",
        "#22K-4080\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "def process_text(text):\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ORIGINAL TEXT:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(text)\n",
        "    print()\n",
        "\n",
        "    # Step 1: Convert to lowercase\n",
        "    text_lower = text.lower()\n",
        "    print(\"Step 1 - Lowercase:\")\n",
        "    print(text_lower)\n",
        "    print()\n",
        "\n",
        "    # Step 2: Remove digits\n",
        "    text_no_digits = re.sub(r'\\d+', '', text_lower)\n",
        "    print(\"Step 2 - Digits Removed:\")\n",
        "    print(text_no_digits)\n",
        "    print()\n",
        "\n",
        "    # Step 3: Remove punctuation\n",
        "    text_no_punct = text_no_digits.translate(str.maketrans('', '', string.punctuation))\n",
        "    print(\"Step 3 - Punctuation Removed:\")\n",
        "    print(text_no_punct)\n",
        "    print()\n",
        "\n",
        "    # Step 4: Trim extra whitespace\n",
        "    text_cleaned = ' '.join(text_no_punct.split())\n",
        "    print(\"Step 4 - Whitespace Trimmed:\")\n",
        "    print(text_cleaned)\n",
        "    print()\n",
        "\n",
        "    # Step 5: Tokenize\n",
        "    tokens = word_tokenize(text_cleaned)\n",
        "    print(\"Step 5 - Tokenized:\")\n",
        "    print(tokens)\n",
        "    print()\n",
        "\n",
        "    # Step 6: Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stop = [word for word in tokens if word not in stop_words]\n",
        "    print(\"Step 6 - Stop Words Removed:\")\n",
        "    print(tokens_no_stop)\n",
        "    print()\n",
        "\n",
        "    # Step 7: Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens_no_stop]\n",
        "    print(\"Step 7 - Stemmed Tokens:\")\n",
        "    print(stemmed_tokens)\n",
        "    print()\n",
        "\n",
        "    # Step 8: Apply lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stop]\n",
        "    print(\"Step 8 - Lemmatized Tokens:\")\n",
        "    print(lemmatized_tokens)\n",
        "    print()\n",
        "\n",
        "    # Step 9: Compare stemming vs lemmatization\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPARISON: STEMMING VS LEMMATIZATION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{'Original':<20} {'Stemmed':<20} {'Lemmatized':<20}\")\n",
        "    print(\"-\" * 60)\n",
        "    for orig, stem, lem in zip(tokens_no_stop, stemmed_tokens, lemmatized_tokens):\n",
        "        if stem != lem:\n",
        "            print(f\"{orig:<20} {stem:<20} {lem:<20} *\")\n",
        "        else:\n",
        "            print(f\"{orig:<20} {stem:<20} {lem:<20}\")\n",
        "    print()\n",
        "    print(\"* indicates different results between stemming and lemmatization\")\n",
        "    print()\n",
        "\n",
        "    # Final output summary\n",
        "    print(\"=\" * 80)\n",
        "    print(\"FINAL OUTPUT SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nCleaned & Tokenized:\")\n",
        "    print(tokens_no_stop)\n",
        "    print(\"\\nStemmed Tokens:\")\n",
        "    print(stemmed_tokens)\n",
        "    print(\"\\nLemmatized Tokens:\")\n",
        "    print(lemmatized_tokens)\n",
        "    print()\n",
        "\n",
        "    return {\n",
        "        'cleaned_tokens': tokens_no_stop,\n",
        "        'stemmed_tokens': stemmed_tokens,\n",
        "        'lemmatized_tokens': lemmatized_tokens\n",
        "    }\n",
        "\n",
        "\n",
        "# Input text\n",
        "input_text = \"\"\"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence! In this lecture, we\n",
        "explored tokenization – splitting text into words or subwords. We also discussed removing\n",
        "punctuation, converting to lowercase, and eliminating stop words like 'is', 'the', and 'an'. Let's\n",
        "normalize text properly before passing it to a model.\"\"\"\n",
        "\n",
        "result = process_text(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBzJUU_9G3Bj",
        "outputId": "7578de06-76a7-4c4f-c133-2e20afb278c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nsJKkNNG5iI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}